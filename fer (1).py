# -*- coding: utf-8 -*-
"""FER.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XXcoZIGRku0QF459V-C0znrUw_VclYzX
"""

from google.colab import drive
drive.mount('/content/drive')

from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Dense,Dropout,Flatten
from keras.layers import Conv2D,MaxPooling2D
import os
import matplotlib.pyplot as plt


train_data_dir='/content/drive/MyDrive/FER/data/train'
validation_data_dir='/content/drive/MyDrive/FER/data/test'


train_datagen = ImageDataGenerator(
					rescale=1./255,
					rotation_range=30,
					shear_range=0.3,
					zoom_range=0.3,
					horizontal_flip=True,
					fill_mode='nearest')

validation_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
					train_data_dir,
					color_mode='grayscale',
					target_size=(48, 48),
					batch_size=32,
					class_mode='categorical',
					shuffle=True)

validation_generator = validation_datagen.flow_from_directory(
							validation_data_dir,
							color_mode='grayscale',
							target_size=(48, 48),
							batch_size=32,
							class_mode='categorical',
							shuffle=True)


class_labels=['Angry','Disgust', 'Fear', 'Happy','Neutral','Sad','Surprise']

img, label = train_generator.__next__()


model = Sequential()

model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))

model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.1))

model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.1))

model.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.1))

model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.2))

model.add(Dense(7, activation='softmax'))

model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])
print(model.summary())


train_path = "/content/drive/MyDrive/FER/data/train"
test_path = "/content/drive/MyDrive/FER/data/test"

num_train_imgs = 0
for root, dirs, files in os.walk(train_path):
    num_train_imgs += len(files)

num_test_imgs = 0
for root, dirs, files in os.walk(test_path):
    num_test_imgs += len(files)

print(num_train_imgs)
print(num_test_imgs)
epochs=50

history=model.fit(train_generator,
                steps_per_epoch=num_train_imgs//32,
                epochs=epochs,
                validation_data=validation_generator,
                validation_steps=num_test_imgs//32)

model.save('model_file.h5')


# 학습 결과 시각화
def plot_training_history(history):
    # 손실 그래프
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    # 정확도 그래프
    plt.subplot(1, 2, 2)
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.show()

plot_training_history(history)

import tensorflow as tf
import cv2
import numpy as np
model = tf.keras.models.load_model('model_file.h5')
frame = cv2.imread("/content/drive/MyDrive/FER/surprised man.jpg")

frame.shape

plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))

faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

gray.shape

faces = faceCascade.detectMultiScale(gray,1.1,4)
for x,y,w,h in faces:
    roi_gray = gray[y:y+h, x:x+w]
    roi_color = frame[y:y+h, x:x+w]
    cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)
    facess = faceCascade.detectMultiScale(roi_gray)
    if len(facess) ==0:
        print("Face not detected")
    else:
        for (ex,ey,ew,eh) in facess:
            face_roi = roi_color[ey:ey+eh, ex:ex+ew]

plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))

plt.imshow(cv2.cvtColor(face_roi, cv2.COLOR_BGR2RGB))

final_image = cv2.resize(face_roi, (48,48))
final_image = cv2.cvtColor(final_image, cv2.COLOR_BGR2GRAY)
final_image = np.expand_dims(final_image, axis=-1)  # Add channel dimension
final_image = np.expand_dims(final_image,axis=0)
final_image = final_image/255.0

Predictions = model.predict(final_image)

Predictions[0]

np.argmax(Predictions)

!pip install deepface
from deepface import DeepFace
import cv2

video_path = "/content/drive/MyDrive/FER/Elon Musk emotion recognition.mp4"
face_model = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")

frame_list = []

capture = cv2.VideoCapture(video_path)

# 원본 비디오의 FPS 얻기
fps = capture.get(cv2.CAP_PROP_FPS)
print(f"Original FPS: {fps}")

from google.colab.patches import cv2_imshow
from deepface import DeepFace

for i in range(100):
  ret, frame = capture.read()
  if not ret:
      break



  face = face_model.detectMultiScale(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY),
                              1.1, 5)
  for x, y, width, height in face:

    emotion = DeepFace.analyze(frame, actions=["emotion"])

    # Print the structure of the returned emotion variable
    print(emotion)

    # If emotion is a list, get the first element
    if isinstance(emotion, list):
      emotion = emotion[0]

    cv2.putText(frame, str(emotion["dominant_emotion"]),
                (x, y + height),
                cv2.FONT_HERSHEY_COMPLEX,
                0.9,
                (255, 255, 0),
                2)

    cv2.rectangle(frame, (x, y),
                  (x + width, y + height),
                  (255, 255, 0),
                  2)

    frame_list.append(frame)

    height, width, colors = frame.shape

    size = (width, height)

    cv2_imshow(frame)

output_path = "Emotions.avi"

output = cv2.VideoWriter(output_path,
                        cv2.VideoWriter_fourcc(*"DIVX"),
                        fps,
                        size)

for frame in range(len(frame_list)):

  output.write(frame_list[frame])

output.release()